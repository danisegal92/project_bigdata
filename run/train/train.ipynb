{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install findspark\n",
    "!pip install confluent-kafka\n",
    "# Downloaded from https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-8-assembly_2.11\n",
    "!wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.0/spark-streaming-kafka-0-8-assembly_2.11-2.4.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # !wget https://raw.githubusercontent.com/grananqvist/Machine-Learning-Web-Application-Firewall-and-Dataset/master/data/payloads.csv\n",
    "!wget https://raw.githubusercontent.com/faizann24/Fwaf-Machine-Learning-driven-Web-Application-Firewall/master/badqueries.txt\n",
    "!wget https://raw.githubusercontent.com/faizann24/Fwaf-Machine-Learning-driven-Web-Application-Firewall/master/goodqueries.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init('/usr/local/spark/spark-2.4.0-bin-hadoop2.7')\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars spark-streaming-kafka-0-8-assembly_2.11-2.4.0.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import CountVectorizerModel, IDFModel, StandardScalerModel, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "from urllib.parse import unquote\n",
    "\n",
    "APP_NAME = \"BigData\"\n",
    "conf = pyspark.SparkConf().setAll([ ('spark.app.name', APP_NAME),\n",
    "                                    ('spark.executor.memory', '8g'),\n",
    "                                    ('spark.cores.max', '2'),\n",
    "                                    ('spark.driver.memory','8g'),\n",
    "                                   ('spark.master', 'local[2]')])\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlc = SQLContext(sc)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = sc.textFile(\"goodqueries.txt\").map(lambda line: Row(is_malicious=0.0,payload=str(unquote(line))[1:])).distinct()\n",
    "bad = sc.textFile(\"badqueries.txt\").map(lambda line: Row(is_malicious=1.0,payload=str(unquote(line))[1:])).distinct()\n",
    "mySchema = StructType([StructField(\"is_malicious\", DoubleType(), True),StructField(\"payload\", StringType(), True)])\n",
    "\n",
    "df = sqlc.createDataFrame(good.union(bad), mySchema).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ngram(payload_obj):\n",
    "    n=2\n",
    "    payload = str(payload_obj)\n",
    "    ngrams = ''\n",
    "    for i in range(0,len(payload)-n + 1):\n",
    "        ngrams += payload[i:i+n]+ ' '\n",
    "    return ngrams[:-1]\n",
    "\n",
    "\n",
    "print('EXAMPLE: bigram of the word <script>:')\n",
    "to_ngram(\"MARAT\")\n",
    "to_ngram(\"DMITRI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_ngrams\n",
    "ngrams = udf(to_ngram, StringType())\n",
    "df = df.withColumn('ngrams', ngrams(df['payload']))\n",
    "\n",
    "# tokenize\n",
    "tokenizer = Tokenizer().setInputCol(\"ngrams\").setOutputCol(\"tokens\")\n",
    "wordsData = tokenizer.transform(df)\n",
    "\n",
    "# vectorize\n",
    "vectorizer = CountVectorizer(inputCol='tokens', outputCol='vectorizer').fit(wordsData)\n",
    "wordsData = vectorizer.transform(wordsData)\n",
    "\n",
    "# calculate scores\n",
    "idf = IDF(minDocFreq=1,inputCol=\"vectorizer\", outputCol=\"tfidf_features\")\n",
    "\n",
    "idf_model = idf.fit(wordsData)\n",
    "wordsData = idf_model.transform(wordsData)\n",
    "\n",
    "wordsData = wordsData.select(\"is_malicious\",\"tfidf_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-452e3168b366>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m scaler = StandardScaler(inputCol=\"tfidf_features\", outputCol=\"scaledFeatures\",\n\u001b[0m\u001b[1;32m      2\u001b[0m                         withStd=True, withMean=False)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Compute summary statistics by fitting the StandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscalerModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordsData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(inputCol=\"tfidf_features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(wordsData)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "wordsData = scalerModel.transform(wordsData).cache()\n",
    "\n",
    "pca = PCA(k=2, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(wordsData.select('is_malicious','scaledFeatures'))\n",
    "result = model.transform(wordsData).select('is_malicious',\"pcaFeatures\").cache()\n",
    "# result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom = np.array(result.filter(result['is_malicious'] == 1.0).rdd.map(lambda x: x['pcaFeatures']).collect()).T\n",
    "norm = np.array(result.filter(result['is_malicious'] == 0.0).rdd.map(lambda x: x['pcaFeatures']).collect()).T\n",
    "\n",
    "fig = plt.figure(figsize = [8,8])\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('PC 1', fontsize = 15)\n",
    "ax.set_ylabel('PC 2', fontsize = 15)\n",
    "ax.set_title('PCA', fontsize = 20)\n",
    "ax.axis([-20, 150, -80, 50])\n",
    "targets = ['Attack','Normal']\n",
    "ax.scatter(anom[0]\n",
    "               , anom[1]\n",
    "               , c = 'r',alpha=0.4)\n",
    "ax.scatter(norm[0]\n",
    "               , norm[1]\n",
    "               , c = 'b'\n",
    "              ,alpha=0.4)\n",
    "\n",
    "ax.legend(targets)\n",
    "ax.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = wordsData.randomSplit([0.8, 0.2], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'scaledFeatures', labelCol = 'is_malicious')\n",
    "\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.maxIter,[10,100,1000]).addGrid(lr.regParam, [0.1, 0.01,0.001])\\\n",
    "    .addGrid(lr.fitIntercept, [False, True]).addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]).build()\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(labelCol='is_malicious'),\n",
    "                           trainRatio=0.8)\n",
    "model = tvs.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.bestModel.transform(test)\n",
    "y_true = predictions.select(['is_malicious']).collect()\n",
    "y_pred = predictions.select(['prediction']).collect()\n",
    "\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues');\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
